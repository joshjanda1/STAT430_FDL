{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw03-assign.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qwaX6gJ1-nUY","colab_type":"text"},"source":["# Overview \n","\n","Please see the [homework policy](https://fdl.thecoatlessprofessor.com/syllabus/#homework)\n","for detailed instructions and some grading notes. Failure to follow instructions\n","will result in point reductions. In particular, make sure to commit each \n","exercise as you complete them. \n","\n","> \"Simple models and a lot of data trump more elaborate models based on less data.\"\n","> \n","> -- Peter Norvig\n","\n","## Grading\n","\n","The rubric CAs will use to grade this assignment is:\n","\n","| Task                                                   | Pts |\n","|:-------------------------------------------------------|----:|\n","| Denessness                                             | 30  |\n","| The Art of Backprop                                    | 20  |\n","| Descent from Below                                     | 10  |\n","| Total                                                  | 60  |\n","\n","\n","## Objectives \n","\n","The objectives behind this homework assignment are as follows:\n","\n","- Implement functions in Python;\n","- Viewing differences in activation functions;\n","- Applying gradient descent; and,\n","- Constructing neural networks."]},{"cell_type":"markdown","metadata":{"id":"FB5p6YDUKaPj","colab_type":"text"},"source":["# Assignment - Homework 3\n","STAT 430 - FDL, Spring 2020\n","\n","Due: **Friday, March 13th, 2020 at 6:00 PM**\n","\n","- **Author:** Josh Janda\n","- **NetID:** joshlj2\n","\n","### Collaborators\n","\n","If you worked with any other student in preparing these answers, please\n","make sure to list their full names and NetIDs (e.g. `FirstName LastName (NetID)` ).\n"]},{"cell_type":"markdown","metadata":{"id":"4Ivn9RloO0w3","colab_type":"text"},"source":["## [30 points] Exercise 1 - Denseness\n","\n","Consider a neural network architecture given by: \n","\n","$$\\begin{align*}\n","z_1^{(1)} &= w_{1,1}^{(1)} x_1 + w_{2,1}^{(1)} x_2 + b_{1}^{(1)} \\\\\n","z_2^{(1)} &= w_{1,2}^{(1)} x_1 + w_{2,2}^{(1)} x_2 + b_{2}^{(1)} \\\\\n","z_3^{(1)} &= w_{1,3}^{(1)} x_1 + w_{2,3}^{(1)} x_2 + b_{3}^{(1)} \\\\\n","a_1^{(1)} &= g^{(1)}\\left({z_1^{(1)}}\\right) \\\\\n","a_2^{(1)} &= g^{(1)}\\left({z_2^{(1)}}\\right) \\\\\n","a_3^{(1)} &= g^{(1)}\\left({z_3^{(1)}}\\right) \\\\\n","z_1^{(2)} &= w_{1,1}^{(2)} a_1^{(1)} + w_{2,1}^{(2)} a_1^{(2)} + w_{3,1}^{(2)} a_1^{(2)} +  b_{1}^{(2)} \\\\\n","a_1^{(2)} &= g^{(2)}\\left({z_1^{(2)}}\\right) \\\\\n","\\hat{y} &= a_1^{(2)}\n","\\end{align*}$$\n","\n","where $g(z)$ refers to a generic activation function.\n","\n","**Note:**\n","\n","- $z_i^{(l)}$ represents the linear combination value at neuron $i$ in layer $l$.\n","- $a_i^{(l)}$ represents the activation value at neuron $i$ in layer $l$.\n","- $w_{j,i}^{(l)}$ represents the weight parameter associated with neuron $j$ in layer $l-1$ to neuron $i$ in layer $l$.\n","- $b_{i}^{(l)}$ represents the bias parameter associated with neuron $i$ in layer $l$.\n","\n","Let the network's **first layer** have weight and bias values of:\n","\n","$$\n","W^{(1)} = \\begin{bmatrix} \n","w_{1,1}^{(1)} & w_{2,1}^{(1)} \\\\\n","w_{1,2}^{(1)} & w_{2,2}^{(1)} \\\\\n","w_{1,3}^{(1)} & w_{2,3}^{(1)} \\\\\n","\\end{bmatrix}_{3 \\times 2} = \\begin{bmatrix} \n","1 & 1 \\\\\n","2 & -2 \\\\\n","3 & -1 \\\\\n","\\end{bmatrix}_{3 \\times 2}, b^{(1)} = \\begin{bmatrix} \n","b_{1}^{(1)} \n","b_{2}^{(1)} \n","b_{3}^{(1)} \n","\\end{bmatrix}_{3 \\times 2} = \\begin{bmatrix} \n","-1 \\\\\n","1 \\\\\n","0 \\\\\n","\\end{bmatrix}_{3 \\times 1}\n","$$\n","\n","Let the network's **second layer** have weight and bias values of:\n","\n","$$\n","W^{(2)} = \\begin{bmatrix} \n","w_{1,1}^{(2)} & w_{2,1}^{(2)} & w_{3,1}^{(2)}\n","\\end{bmatrix}_{1 \\times 3} = \\begin{bmatrix} \n","2 & 3 & 1\n","\\end{bmatrix}_{1 \\times 3},\\, b^{(2)} = \\begin{bmatrix} \n","b_{1}^{(2)} \n","\\end{bmatrix}_{1 \\times 1} = \\begin{bmatrix} \n","-1\n","\\end{bmatrix}_{1 \\times 1}\n","$$\n","\n","Moreover, let the **data** $x$ be: \n","\n","$$\n","x = \\begin{bmatrix} \n","x_1 \\\\\n","x_2 \n","\\end{bmatrix}_{2 \\times 1} = \n","\\begin{bmatrix} \n","4 \\\\\n","1 \n","\\end{bmatrix}_{2 \\times 1}\n","$$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GXf6VwekdJg9","colab_type":"text"},"source":["**(a) (10 points)** Sketch the neural network architecture.\n"]},{"cell_type":"markdown","metadata":{"id":"T83MnD2kTDqJ","colab_type":"text"},"source":["![](https://i.imgur.com/t9GPo4h.jpg)"]},{"cell_type":"markdown","metadata":{"id":"j5RjQuUoTJ8u","colab_type":"text"},"source":["**(b) (10 points)** Consider the generic activation function $g(x)$ equivalent to the identity function $g(x) = x$. What would be the output of the network be?\n","What does a linear activation function imply about the network's architecture?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YjaFxI6eaikg","colab_type":"text"},"source":["$$z^{(1)}_1 = 4(1) + 1(1) + -1 = 4$$\n","$$z^{(1)}_2 = 4(2) + 1(-2) + 1 = 7$$\n","$$z^{(1)}_3 = 4(3) + 1(-1) + 0 = 11$$\n","\n","$$z^{(1)} = \\begin{bmatrix} \n","4 \\\\\n","7 \\\\\n","11 \\\\\n","\\end{bmatrix}_{3 \\times 1}\n","$$\n","\n","$$a^{(1)}_1 = g(z^{(1)}_1) = 4$$\n","$$a^{(1)}_2 = g(z^{(1)}_2) = 7$$\n","$$a^{(1)}_3 = g(z^{(1)}_3) = 11$$\n","\n","$$a^{(1)} = \\begin{bmatrix} \n","4 \\\\\n","7 \\\\\n","11 \\\\\n","\\end{bmatrix}_{3 \\times 1}\n","$$\n","\n","$$z^{(2)}_1 = 4(2) + 7(3) + 11(1) + -1 = 39$$\n","$$a^{(2)}_1 = g(z^{(2)}_1) = 39$$\n","$$\\hat y = a^{(2)}_1 = 39$$\n","\n","\n","\n","With the generic activation function $g(x) = x$, the output of the network is 39.\n","\n","With a linear activation function, this implies that we have simply just created a linear regression model. With a linear activation function, we can write the network as just one layer."]},{"cell_type":"markdown","metadata":{"id":"I5GSxBoSajDz","colab_type":"text"},"source":["**(c) (10 points)** What would the output be if the generic activation function $g(x)$ is set to be equivalent to PReLU function with $\\alpha = 0.01$ in the **first layer**, $g^{(1)}(x) = \\mathrm{PReLU}(x, \\alpha = 0.01)$, and the ReLU function in the **second layer**, $g^{(2)}(x) = \\mathrm{ReLU}(x)$.\n"]},{"cell_type":"markdown","metadata":{"id":"8CWtTnzubTYn","colab_type":"text"},"source":["\n","\n","$$z^{(1)}_1 = 4(1) + 1(1) + -1 = 4$$\n","$$z^{(1)}_2 = 4(2) + 1(-2) + 1 = 7$$\n","$$z^{(1)}_3 = 4(3) + 1(-1) + 0 = 11$$\n","\n","$$z^{(1)} = \\begin{bmatrix} \n","4 \\\\\n","7 \\\\\n","11 \\\\\n","\\end{bmatrix}_{3 \\times 1}\n","$$\n","\n","$$a^{(1)}_1 = g^{(1)}(z^{(1)}_1) = 4$$\n","$$a^{(1)}_2 = g^{(1)}(z^{(1)}_2) = 7$$\n","$$a^{(1)}_3 = g^{(1)}(z^{(1)}_3) = 11$$\n","\n","$$a^{(1)} = \\begin{bmatrix} \n","4 \\\\\n","7 \\\\\n","11 \\\\\n","\\end{bmatrix}_{3 \\times 1}\n","$$\n","\n","$$z^{(2)}_1 = 4(2) + 7(3) + 11(1) + -1 = 39$$\n","$$a^{(2)}_1 = g^{(2)}(z^{(2)}_1) = 39$$\n","$$\\hat y = a^{(2)}_1 = 39$$\n","\n","With $g^{(1)} = PReLU(x, \\alpha=0.01)$, we obtain the same neuron outputs for the first layer as no $z^{(1)}$ values are less than zero. With $g^{(2)} = ReLU(x)$, we also obtain the same neuron output for this layer as $z^{(2)}$ does not have a value less than zero.\n","\n","Overall, we obtain the same output as the above question where g(x) = x, but now with non-linear activation functions therefore making these changes in activation functions resulting in a neural network that cannot be written as a linear regression."]},{"cell_type":"markdown","metadata":{"id":"sMqtA6xjbrOJ","colab_type":"text"},"source":["## [20 points] Exercise 2 - The Art of Backprop\n","\n","Recall the network architecture given in **Exercise 1**. Let the generic\n","activation function $g(x)$ be equivalent to the identity, $g(x) = x$, **across all layers**.\n","\n","**(a) [15 points]** Let the cost function for the network be squared-error given as: \n","\n","$$J(W) = \\left({y - \\hat y }\\right)^2$$\n","\n","Compute the partial derivatives with respect to:\n","\n","$\\frac{\\partial J(W)}{\\partial w_{1,1}^{(2)} }$, \n","$\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)} }$, \n","$\\frac{\\partial J(W)}{\\partial b_{3}^{(1)} }$\n","\n","_Hint:_ Recall **Exercise 1 b** computation for $\\hat y$.\n"]},{"cell_type":"markdown","metadata":{"id":"m5c1qcEnexIu","colab_type":"text"},"source":["$$\\frac{\\partial J(W)}{\\partial w_{1,1}^{(2)}} = \\frac{\\partial J(W)}{\\partial a_{1}^{(2)}}*\\frac{\\partial a_{1}^{(2)}}{\\partial z_{1}^{(2)}}*\\frac{\\partial z_{1}^{(2)}}{\\partial w_{1,1}^{(2)}}$$\n","\n","Note that $J(W)$ can be written as due to $g(x)=x$ activation function:\n","\n","$$J(W) = [y-(w_1^{(2)}a_1^{(1)}+w_2^{(2)}a_2^{(1)}+w_3^{(2)}a_3^{(1)}+b_1^{(2)})]^2$$\n","\n","With $J(W)$ written as that above calculation, we can now simply compute $\\frac{\\partial J(W)}{\\partial w_{1,1}^{(2)}}$\n","\n","$$\\frac{\\partial J(W)}{\\partial w_{1,1}^{(2)}} = 2(y - w_1^{(2)}a_1^{(1)} - w_2^{(2)}a_2^{(1)} - w_3^{(2)}a_3^{(1)} - b_1^{(2)})*(-a_1^{(1)})$$\n","________________________________________________________________________________\n","\n","$$\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)}} = \\frac{\\partial J(W)}{\\partial a_{1}^{(2)}}*\\frac{\\partial a_{1}^{(2)}}{\\partial z_{1}^{(2)}}*\\frac{\\partial z_{1}^{(2)}}{\\partial a_{2}^{(1)}}*\\frac{\\partial a_{2}^{(1)}}{\\partial z_{2}^{(1)}}*\\frac{\\partial z_{2}^{(1)}}{\\partial w_{1, 2}^{(1)}}$$\n","\n","Note that $J(W)$ can be written as due to $g(x)=x$ activation function:\n","\n","$$J(W) = [y-(w_1^{(2)}(w_{1,1}^{(1)}x_1 + w_{2,1}^{(1)}x_2 + b_{1}^{(1)}) + w_2^{(2)}(w_{1,2}^{(1)}x_1 + w_{2,2}^{(1)}x_2 + b_{2}^{(1)}) + w_3^{(2)}(w_{1,3}^{(1)}x_1 + w_{2,3}^{(1)}x_2 + b_{3}^{(1)}) + b_1^{(2)})]^2$$\n","\n","With $J(W)$ written as that above calculation, we can now simply compute $\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)}}$ which is equal to:\n","\n","$$2(y-(w_1^{(2)}(w_{1,1}^{(1)}x_1 + w_{2,1}^{(1)}x_2 + b_{1}^{(1)}) + w_2^{(2)}(w_{1,2}^{(1)}x_1 + w_{2,2}^{(1)}x_2 + b_{2}^{(1)}) + w_3^{(2)}(w_{1,3}^{(1)}x_1 + w_{2,3}^{(1)}x_2 + b_{3}^{(1)}) + b_1^{(2)}))(-w_{2}^{(2)}x_1)$$\n","________________________________________________________________________________\n","\n","$$\\frac{\\partial J(W)}{\\partial b_{3}^{(1)}} = \\frac{\\partial J(W)}{\\partial a_{1}^{(2)}}*\\frac{\\partial a_{1}^{(2)}}{\\partial z_{1}^{(2)}}*\\frac{\\partial z_{1}^{(2)}}{\\partial a_{3}^{(1)}}*\\frac{\\partial a_{3}^{(1)}}{\\partial z_{3}^{(1)}}*\\frac{\\partial z_{3}^{(1)}}{\\partial b_{3}^{(1)}}$$\n","\n","Note that $J(W)$ can be written as due to $g(x)=x$ activation function:\n","\n","$$J(W) = [y-(w_1^{(2)}(w_{1,1}^{(1)}x_1 + w_{2,1}^{(1)}x_2 + b_{1}^{(1)}) + w_2^{(2)}(w_{1,2}^{(1)}x_1 + w_{2,2}^{(1)}x_2 + b_{2}^{(1)}) + w_3^{(2)}(w_{1,3}^{(1)}x_1 + w_{2,3}^{(1)}x_2 + b_{3}^{(1)}) + b_1^{(2)})]^2$$\n","\n","With $J(W)$ written as that above calculation, we can now simply compute $\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)}}$ which is equal to:\n","\n","$$2(y-(w_1^{(2)}(w_{1,1}^{(1)}x_1 + w_{2,1}^{(1)}x_2 + b_{1}^{(1)}) + w_2^{(2)}(w_{1,2}^{(1)}x_1 + w_{2,2}^{(1)}x_2 + b_{2}^{(1)}) + w_3^{(2)}(w_{1,3}^{(1)}x_1 + w_{2,3}^{(1)}x_2 + b_{3}^{(1)}) + b_1^{(2)}))(-w_{3}^{(2)})$$"]},{"cell_type":"markdown","metadata":{"id":"PSkFDUzJezsN","colab_type":"text"},"source":["**(b) [5 points]** If $y = 42$, compute the realized value for each of the partial derivatives given in **(a)**.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NXlgQpbAibQ6","colab_type":"text"},"source":["$$(\\frac{\\partial J(W)}{\\partial w_{1,1}^{(2)}}|y=42)= 2(42 - 2(4) - 3(7)- 1(11) - (-1))*(-4) = -24$$\n","\n","$$(\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)}}|y=42)= 2(42 - (2(1*4 + 1*1 + -1) + 3(2*4 + -2*1 + 1) + 1(3*4 + -1*1 + 0) + -1))*(-3*4) = -72$$\n","\n","$$(\\frac{\\partial J(W)}{\\partial b_{3}^{(1)}}|y=42)= 2(42 - (2(1*4 + 1*1 + -1) + 3(2*4 + -2*1 + 1) + 1(3*4 + -1*1 + 0) + -1))*(-1) = -6$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SYsHcfcHihIV","colab_type":"text"},"source":["## [10 points] Exercise 3 - Descent from Below\n","\n","Recall the partial derivatives obtained in **Exercise 2**. \n","\n","**(a) [5 points]** Write what the parameter updates would be under **SGD with Momentum** for: \n","\n","$\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)} }$ and  $\\frac{\\partial J(W)}{\\partial b_{3}^{(1)} }$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bmkXdIJGk7Xz","colab_type":"text"},"source":["For $w_{1, 2}^{(1)}$:\n","\n","Starting with $v_t = 0$, for the first iteration:\n","\n","$$v_t = \\rho*0 - \\alpha*\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)}}$$\n","\n","We then update $w_{1,2}^{(1)}$ with the paramater update equation:\n","\n","$$w_{1,2}^{(1)} = w_{1,2}^{(1)} + v_t$$\n","\n","For the next iteration and onwards:\n","\n","$$v_t = \\rho*v_{t-1} - \\alpha*\\frac{\\partial J(W)}{\\partial w_{1,2}^{(1)}}$$\n","\n","The weight $w_{1, 2}^{(1)}$ is then updated using the parameter update equation found above. This is done for each iteration given *n* iterations.\n","\n","For $b_{3}^{(1)}$:\n","\n","$$v_t = \\rho*0 - \\alpha*\\frac{\\partial J(W)}{\\partial b_{3}^{(1)}}$$\n","\n","We then update $b_{3}^{(1)}$ with the paramater update equation:\n","\n","$$b_{3}^{(1)} = b_{3}^{(1)} + v_t$$\n","\n","For the next iteration and onwards:\n","\n","$$v_t = \\rho*v_{t-1} - \\alpha*\\frac{\\partial J(W)}{\\partial b_{3}^{(1)}}$$\n","\n","The weight $b_{3}^{(1)}$ is then updated using the parameter update equation found above. This is done for each iteration given *n* iterations."]},{"cell_type":"markdown","metadata":{"id":"lWo_Y51UkuAJ","colab_type":"text"},"source":["**(b) [5 points]** Compute the parameter updates given $\\alpha = 0.5$, $\\rho = 0.1$, and $v_0 = 4$.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-c506otwk-5j","colab_type":"text"},"source":["For $w_{1, 2}^{(1)}$:\n","\n","$$v_t = .1*4 - 0.5*-72 = 36.4$$\n","\n","$$w_{1,2}^{(1)} = 2 + 36.4 = 38.4$$\n","\n","For $b_3^{(1)}$:\n","\n","$$v_t = .1*4 - 0.5*-6 = 3.4$$\n","\n","$$b_3^{(1)} = 0 + 3.4 = 3.4$$\n"]}]}